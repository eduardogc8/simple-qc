{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code blocs bellow in sequence. You can read the descriptions to understand it.\n",
    "\n",
    "\n",
    "The dependencies can be found in https://github.com/eduardogc8/simple-qc\n",
    "\n",
    "Before starting to run the experiments, change the variable ``path_wordembedding``, in the code block below, for the correct directory path. Make sure that the word embedding inside follow the template `wiki.multi.*.vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/eduardo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from benchmarking_methods import run_benchmark\n",
    "from building_classifiers import lstm_default, svm_linear, random_forest, cnn\n",
    "from download_word_embeddings import muse_embeddings_path, download_if_not_existing\n",
    "from loading_data import load_embedding, load_uiuc\n",
    "\n",
    "path_wordembedding = muse_embeddings_path\n",
    "download_if_not_existing()\n",
    "from benchmarking_methods import run_benchmark_cv\n",
    "from feature_creation import create_feature\n",
    "from loading_data import load_disequa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *create_features* transform the questions in numerical vector to a classifier model.<br>It returns the output in the df_2 dataframe that is a parameter (*df_2.feature_type*, according to the *feature_type*).<br><br>\n",
    "**feature_type:** type of feature. (bow, tfidf, embedding, embedding_sum, vocab_index, pos_index, pos_hotencode, ner_index, ner_hotencode)<br> \n",
    "**df:** the dataframe used to fit the transformers models (df.questions).<br>\n",
    "**df_2:** dataframe wich the data will be transformed (df_2.questions).<br>\n",
    "**embedding:** embedding model for word embedding features type.<br>\n",
    "**max_features:** used in bag-of-words and TFIDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "",
    "\n",
    "### Create classifier models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are created through functions that return them. These functions will be used to create a new model in each experiment. Therefore, an instance of a model is created by the benchmark function and not explicitly in a code block.",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Load UIUC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DISEQuA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark UIUC - Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal:** it uses the default fixed split of UIUC between train dataset (at last 5500 instances) and test dataset (500 instances). Therefore, it does not use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the *run_benchmark* function is executed, it will save each result in the *save* path.\n",
    "\n",
    "**model:** a dictionary with the classifier name and the function to create and return the model (not an instance of the model). <br> Example: *model = {'name': 'SVM', 'model': svm_linear}*<br>\n",
    "**X:** all the training set.<br>\n",
    "**y:** all the labels of the training set.<br>\n",
    "**x_test:** test set.<br>\n",
    "**y_test:** labels of the test set.<br>\n",
    "**sizes_train:** sizes of training set. For each size, an experiment is executed.<br>\n",
    "**runs:** number of time that each experiment is executed (used in models which has parameters with random values, like weights in an ANN).<br>\n",
    "**save:** csv path where the results will be saved.<br>\n",
    "**metric_average:** used in f1, recall and precision metrics<br>\n",
    "**onehot:** one-hot model to transform labels.<br>\n",
    "**out_dim:** the total of classes for ANN models.<br>\n",
    "**epochs:** epochs for ANN models.<br>\n",
    "**batch_size:** batch_size for ANN models.<br>\n",
    "**vocabulary_size:** vocabulary size (used in CNN model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "",
    "\n",
    "## Benchmark UIUC and DISEQuA - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation:** instead of uses default fixed splits, it uses the all the dataset with cross-validation with 10 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the *run_benchmark* function is executed, it will save each result in the *save* path.\n",
    "\n",
    "**model:** a dictionary with the classifier name and the function to create and return the model (not an instance of the model). <br> Example: *model = {'name': 'SVM', 'model': svm_linear}*<br>\n",
    "**X:** Input features.<br>\n",
    "**y:** Input labels.<br>\n",
    "**sizes_train:** sizes of training set. For each size, an experiment is executed.<br>\n",
    "**folds:** Amount of folds for cross-validations.<br>\n",
    "**save:** csv path where the results will be saved.<br>\n",
    "**metric_average:** used in f1, recall and precision metrics<br>\n",
    "**onehot:** one-hot model to transform labels.<br>\n",
    "**epochs:** epochs for ANN models.<br>\n",
    "**batch_size:** batch_size for ANN models.<br>\n",
    "**vocabulary_size:** vocabulary size (used in CNN model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "",
    "\n",
    "## Run UIUC Benchmark - Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different classifier models are tested with different dependency levels of external linguistic resources (Low, Medium and High)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 0.41161417961120605\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 0.5091326236724854\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 0.4507761001586914\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, max_features=2000)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, max_features=2000)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf_train = np.array([list(r) for r in dataset_train['tfidf'].values])\n",
    "    tfidf_test = np.array([list(r) for r in dataset_test['tfidf'].values])\n",
    "    tfidf_train = normalize(tfidf_train, norm='max')\n",
    "    tfidf_test = normalize(tfidf_test, norm='max')\n",
    "    \n",
    "    X_train = np.array([list(x) for x in dataset_train['tfidf'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['tfidf'].values])\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  save='results/UIUC_svm_tfidf_' + language + '.csv', runs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 12.742478370666504\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 14.3670494556427\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 13.426743984222412\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, max_features=2000)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, max_features=2000)\n",
    "    create_feature('embedding_sum', None, dataset_train, embedding)\n",
    "    create_feature('embedding_sum', None, dataset_test, embedding)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf_train = np.array([list(r) for r in dataset_train['tfidf'].values])\n",
    "    tfidf_test = np.array([list(r) for r in dataset_test['tfidf'].values])\n",
    "    tfidf_train = normalize(tfidf_train, norm='max')\n",
    "    tfidf_test = normalize(tfidf_test, norm='max')\n",
    "    \n",
    "    embedding_train = np.array([list(r) for r in dataset_train['embedding_sum'].values])\n",
    "    embedding_test = np.array([list(r) for r in dataset_test['embedding_sum'].values])\n",
    "    embedding_train = normalize(embedding_train, norm='max')\n",
    "    embedding_test = normalize(embedding_test, norm='max')\n",
    "    \n",
    "    X_train = np.array([list(x) + list(xx) for x, xx in zip(tfidf_train, embedding_train)])\n",
    "    X_test = np.array([list(x) + list(xx) for x, xx in zip(tfidf_test, embedding_test)])\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500], \n",
    "                  runs=1, save='results/UIUC_svm_cortes_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB + POS + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 12.932790994644165\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 15.48978304862976\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 14.322027683258057\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, max_features=2000)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, max_features=2000)\n",
    "    create_feature('embedding_sum', dataset_train, dataset_train, embedding)\n",
    "    create_feature('embedding_sum', dataset_train, dataset_test, embedding)\n",
    "    create_feature('pos_hotencode', dataset_train, dataset_train)\n",
    "    create_feature('pos_hotencode', dataset_train, dataset_test)\n",
    "    create_feature('ner_hotencode', dataset_train, dataset_train)\n",
    "    create_feature('ner_hotencode', dataset_train, dataset_test)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf_train = np.array([list(r) for r in dataset_train['tfidf'].values])\n",
    "    tfidf_test = np.array([list(r) for r in dataset_test['tfidf'].values])\n",
    "    tfidf_train = normalize(tfidf_train, norm='max')\n",
    "    tfidf_test = normalize(tfidf_test, norm='max')\n",
    "    \n",
    "    embedding_train = np.array([list(r) for r in dataset_train['embedding_sum'].values])\n",
    "    embedding_test = np.array([list(r) for r in dataset_test['embedding_sum'].values])\n",
    "    embedding_train = normalize(embedding_train, norm='max')\n",
    "    embedding_test = normalize(embedding_test, norm='max')\n",
    "    \n",
    "    pos_train = np.array([list(r) for r in dataset_train['pos_hotencode'].values])\n",
    "    pos_test = np.array([list(r) for r in dataset_test['pos_hotencode'].values])\n",
    "    \n",
    "    ner_train = np.array([list(r) for r in dataset_train['ner_hotencode'].values])\n",
    "    ner_test = np.array([list(r) for r in dataset_test['ner_hotencode'].values])\n",
    "    \n",
    "    X_train = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf_train, embedding_train, pos_train, ner_train)])\n",
    "    X_test = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf_test, embedding_test, pos_test, ner_test)])\n",
    "    \n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    \n",
    "    classes = list(dataset_train['class'].unique())\n",
    "    y_train_ = [classes.index(c) for c in y_train]\n",
    "    \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=1, save='results/UIUC_svm_high_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UIUC Benchmark - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different classifier models are tested with different dependency levels of external linguistic resources (Low, Medium and High)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 22.216983318328857\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 24.743942499160767\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 22.218426942825317\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset = pd.concat([dataset_train, dataset_test])\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, [50, 100] + list(range(500, 5501, 500)),\n",
    "                     save='results/UIUC_cv_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 270.81999158859253\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 329.85390615463257\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 327.71513843536377\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset = pd.concat([dataset_train, dataset_test])\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', None, dataset, embedding)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) for x, xx in zip(tfidf, embedding)])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, [50, 100] + list(range(500, 5501, 500)),\n",
    "                     save='results/UIUC_cv_svm_cortes_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB + POS + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 300.1998710632324\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 361.68490052223206\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 308.6705446243286\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset = pd.concat([dataset_train, dataset_test])\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', dataset, dataset, embedding)\n",
    "    create_feature('pos_hotencode', dataset, dataset)\n",
    "    create_feature('ner_hotencode', dataset, dataset)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    pos = np.array([list(r) for r in dataset['pos_hotencode'].values])\n",
    "    \n",
    "    ner = np.array([list(r) for r in dataset['ner_hotencode'].values])\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf, embedding, pos, ner)])\n",
    "    \n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, [50, 100] + list(range(500, 5501, 500)),\n",
    "                     save='results/UIUC_cv_svm_high_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DISEQuA Benchmark - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different classifier models are tested with different dependency levels of external linguistic resources (Low, Medium and High)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + <font color=#007700>TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.027012586593628\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.0114972591400146\n",
      "\n",
      "\n",
      "Language:  it\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.1434721946716309\n",
      "\n",
      "\n",
      "Language:  nl\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.1250619888305664\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'it', 'nl']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, sizes_train=[100,200,300,400],\n",
    "                     save='results/DISEQuA_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + <font color=#007700>TF-IDF</font> + <font color=#0055CC>WB</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.358882427215576\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 7.197380065917969\n",
      "\n",
      "\n",
      "Language:  it\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 5.5334153175354\n",
      "\n",
      "\n",
      "Language:  nl\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.624628782272339\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'it', 'nl']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', None, dataset, embedding)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) for x, xx in zip(tfidf, embedding)])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, sizes_train=[100,200,300,400],\n",
    "                     save='results/DISEQuA_svm_cortes_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + <font color=#007700>TF-IDF</font> + <font color=#0055CC>WB</font> + <font color=#CC6600>POS</font> + <font color=#CC6600>NER</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.811999559402466\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 8.384974479675293\n",
      "\n",
      "\n",
      "Language:  it\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.426969528198242\n",
      "\n",
      "\n",
      "Language:  nl\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.852076053619385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for language in ['en', 'es', 'it', 'nl']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', dataset, dataset, embedding)\n",
    "    create_feature('pos_hotencode', dataset, dataset)\n",
    "    create_feature('ner_hotencode', dataset, dataset)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    pos = np.array([list(r) for r in dataset['pos_hotencode'].values])\n",
    "    \n",
    "    ner = np.array([list(r) for r in dataset['ner_hotencode'].values])\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf, embedding, pos, ner)])\n",
    "    \n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, sizes_train=[100,200,300,400],\n",
    "                     save='results/DISEQuA_svm_high_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old stuffs bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'en', 'es'\n",
    "for language in ['es']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    #embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    text_representation = 'vocab_index'\n",
    "    vocabulary_inv = create_feature(text_representation, dataset_train, dataset_train)\n",
    "    create_feature(text_representation, dataset_train, dataset_test)\n",
    "    model = {'name': 'cnn', 'model': cnn}\n",
    "    X_train = np.array([list(x) for x in dataset_train[text_representation].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test[text_representation].values])\n",
    "    #X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    #X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray()\n",
    "    # , \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=30, save='results/UIUC_cnn_' + language + '.csv', epochs=100, onehot=ohe,\n",
    "                  vocabulary_size=len(vocabulary_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  es\n",
      "(100,)\n",
      "object\n",
      "(1349,)\n",
      "\n",
      "1000|...\n",
      "2000|...\n",
      "3000|...\n",
      "4000|...\n",
      "5500|...\n",
      "Run time benchmark: 228.79835891723633\n"
     ]
    }
   ],
   "source": [
    "for language in ['es']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset_train = dataset_train[:100]\n",
    "    #dataset_test = dataset_test[:10]\n",
    "    create_feature('embedding', dataset_train, dataset_train, embedding)\n",
    "    create_feature('embedding', dataset_train, dataset_test, embedding)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    #print(dataset_train['embedding'].values.shape)\n",
    "    #print(dataset_train['embedding'].values.dtype)\n",
    "    #print(dataset_test['embedding'].values.shape)\n",
    "    X_train = np.array([list(x) for x in dataset_train['embedding'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['embedding'].values])\n",
    "    X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "#     y_train_sub = dataset_train['sub_class'].values\n",
    "#     sub_classes = set()\n",
    "#     for sc in y_train_sub:\n",
    "#         sub_classes.add(sc)\n",
    "#     y_test_sub = dataset_test['sub_class'].values\n",
    "#     X_test_sub_ = []\n",
    "#     y_test_sub_ = []\n",
    "#     for i in range(len(X_test)):\n",
    "#         if y_train_sub[i] in sub_classes:\n",
    "#             X_test_sub_.append(X_test[i])\n",
    "#             y_test_sub_.append(y_train_sub[i])\n",
    "#     X_test_sub_ = np.array(X_test_sub_)\n",
    "#     y_test_sub_ = np.array(y_test_sub_)\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray() \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=30, save='results/UIUC_lstm_embedding_' + language + '_2.csv', epochs=100, onehot=ohe)\n",
    "    #run_benchmark(model, X_train, y_train_sub, X_test_sub_, y_test_sub_, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "    #              save='results/UIUCsub_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['en']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    #embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    # debug\n",
    "    print('WARNING: use subset (first 1000 entries) of training data')\n",
    "    #dataset_train = dataset_train[:5500].copy()\n",
    "    \n",
    "    create_feature('bert', dataset_train, dataset_train)\n",
    "    create_feature('bert', dataset_train, dataset_test)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    X_train = dataset_train['bert'].values\n",
    "    X_test = dataset_test['bert'].values\n",
    "    \n",
    "    X_train = np.array([x for x in X_train])\n",
    "    X_test = np.array([x for x in X_test])\n",
    "    \n",
    "    #X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    #X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray() \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[5500], # 1000, 2000, 3000, 4000, 5500\n",
    "                  runs=1, save='results/UIUC_lstm_bert_' + language + '.csv', \n",
    "                  epochs=100, onehot=ohe, in_dim=768)\n",
    "    #run_benchmark(model, X_train, y_train_sub, X_test_sub_, y_test_sub_, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "    #              save='results/UIUCsub_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, embedding)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400,405],\n",
    "                  save='results/DISEQuA_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFC + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, embedding)\n",
    "    model = {'name': 'rfc', 'model': random_forest}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_rfc_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM + TFIDF_3gram + SKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf_3gram', dataset, dataset)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    skb = SelectKBest(chi2, k=2000).fit(X, y)\n",
    "    X = skb.transform(X)\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_svm_tfidf_3gram_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language, embd_l in zip(['SPA'], ['es']):\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + embd_l + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('embedding', dataset, dataset, embedding)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    X = np.array([list(x) for x in dataset['embedding'].values])\n",
    "    y = dataset['class'].values\n",
    "    X = pad_sequences(X, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    ohe = OneHotEncoder()\n",
    "    y = ohe.fit_transform([[y_] for y_ in y]).toarray()\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400,405], onehot=ohe,\n",
    "                  save='results/DISEQuA_lstm_embedding_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language, embd_l in zip(['DUT', 'ENG', 'ITA', 'SPA'], ['nl', 'eng', 'it', 'es']):\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    #embedding = load_embedding(path_wordembedding + 'wiki.multi.' + embd_l + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    text_representation = 'vocab_index'\n",
    "    vocabulary_inv = create_feature(text_representation, dataset, dataset)\n",
    "    model = {'name': 'cnn', 'model': cnn}\n",
    "    X = np.array([list(x) for x in dataset[text_representation].values])\n",
    "    y = dataset['class'].values\n",
    "    #X = pad_sequences(X, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    ohe = OneHotEncoder()\n",
    "    y = ohe.fit_transform([[y_] for y_ in y]).toarray()\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400], onehot=ohe, vocabulary_size=len(vocabulary_inv),\n",
    "                  save='results/DISEQuA_cnn_' + language + '.csv', epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
