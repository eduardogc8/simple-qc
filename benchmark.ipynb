{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code blocs bellow in sequence. You can read the descriptions to understand it.\n",
    "\n",
    "\n",
    "The dependencies can be found in https://github.com/eduardogc8/simple-qc\n",
    "\n",
    "Before starting to run the experiments, change the variable ``path_wordembedding``, in the code block below, for the correct directory path. Make sure that the word embedding inside follow the template `wiki.multi.*.vec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package punkt to /home/eduardo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Bidirectional, MaxPooling1D, Flatten, concatenate, GlobalMaxPooling1D, Concatenate\n",
    "from keras.layers import Dense, Dropout, LSTM, TimeDistributed,Conv1D,Embedding, Reshape, Conv2D, MaxPool2D\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVR\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import Counter\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import random\n",
    "import keras\n",
    "import nltk\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "\n",
    "path_wordembedding = '/home/eduardo/word_embedding/'\n",
    "#path_wordembedding = '/mnt/DATA2/NLP/MUSE_wordembeddings/'\n",
    "nltk.download('punkt')\n",
    "\n",
    "cache = {}\n",
    "\n",
    "embedding_dt = None\n",
    "embedding_en = None\n",
    "embedding_es = None\n",
    "embedding_it = None\n",
    "embedding_pt = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *create_features* transform the questions in numerical vector to a classifier model.<br>It returns the output in the df_2 dataframe that is a parameter (*df_2.feature_type*, according to the *feature_type*).<br><br>\n",
    "**feature_type:** type of feature. (bow, tfidf, embedding, embedding_sum, vocab_index, pos_index, pos_hotencode, ner_index, ner_hotencode)<br> \n",
    "**df:** the dataframe used to fit the transformers models (df.questions).<br>\n",
    "**df_2:** dataframe wich the data will be transformed (df_2.questions).<br>\n",
    "**embedding:** embedding model for word embedding features type.<br>\n",
    "**max_features:** used in bag-of-words and TFIDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(feature_type, df, df_2, embedding=None, max_features=5000):\n",
    "    \n",
    "    # Bag of Words\n",
    "    if feature_type == 'bow':\n",
    "        model = CountVectorizer(analyzer='word', strip_accents=None, \n",
    "                                ngram_range=(1, 1), lowercase=True, \n",
    "                                max_features=max_features)\n",
    "        model.fit(df['question'])\n",
    "        ret = model.transform(df_2['question']).toarray()\n",
    "        df_2['bow'] = [x for x in ret]\n",
    "\n",
    "    # TF-IDF\n",
    "    if feature_type == 'tfidf':\n",
    "        model = TfidfVectorizer(analyzer='word', strip_accents=None, \n",
    "                                ngram_range=(1, 1), lowercase=True, \n",
    "                                max_features=max_features)\n",
    "        model.fit(df['question'])\n",
    "        ret = model.transform(df_2['question']).toarray()\n",
    "        df_2['tfidf'] = [x for x in ret]\n",
    "    \n",
    "    # Word embedding (used in LSTM)\n",
    "    if feature_type == 'embedding':\n",
    "        if embedding is None:\n",
    "            print('Error: embedding is None')\n",
    "            return\n",
    "        embds = []\n",
    "        for question in df_2['question']:\n",
    "            tokens = nltk.word_tokenize(question.replace(\"多\", \"\"))\n",
    "            embed = []\n",
    "            for token in tokens:\n",
    "                if token.lower() in embedding:\n",
    "                    embed.append(embedding[token.lower()])\n",
    "                else:\n",
    "                    embed.append(np.zeros(300))\n",
    "            embds.append(embed)\n",
    "        df_2['embedding'] = embds\n",
    "    \n",
    "    # Word embedding or Sentence embedding (sum the vector)\n",
    "    if feature_type == 'embedding_sum':\n",
    "        if embedding is None:\n",
    "            print('Error: embedding is None')\n",
    "            return\n",
    "        embds = []\n",
    "        model = MeanEmbeddingVectorizer(embedding)\n",
    "        # model.fit(df['question'])\n",
    "        questions = [nltk.word_tokenize(question.replace(\"多\", \"\")) for question in df_2['question']]\n",
    "        ret = model.transform(questions)\n",
    "        df_2['embedding_sum'] = [x for x in ret]\n",
    "    \n",
    "    # Vocabulary index (used in CNN)\n",
    "    if feature_type == 'vocab_index':\n",
    "        questions_split = [nltk.word_tokenize(q.replace(\"多\", \"\")) for q in df['question']]\n",
    "        questions_split_2 = [nltk.word_tokenize(q.replace(\"多\", \"\")) for q in df_2['question']]\n",
    "        padding_word = \"<PAD/>\"\n",
    "        padded_questions = []\n",
    "        padded_questions_2 = []\n",
    "        sequence_length = max(len(x) for x in questions_split)\n",
    "        for i in range(len(questions_split)):\n",
    "            question = questions_split[i]\n",
    "            num_padding = sequence_length - len(question)\n",
    "            new_question = question + [padding_word] * num_padding\n",
    "            padded_questions.append(new_question)\n",
    "        word_counts = Counter(itertools.chain(*padded_questions))\n",
    "        vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "        vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "        vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "        for i in range(len(questions_split_2)):\n",
    "            question = questions_split_2[i]\n",
    "            nq = []\n",
    "            for w in question:\n",
    "                if w in vocabulary:\n",
    "                    nq.append(w)\n",
    "                else:\n",
    "                    nq.append(padding_word)\n",
    "            question = nq\n",
    "            num_padding = sequence_length - len(question)\n",
    "            new_question = question + [padding_word] * num_padding\n",
    "            padded_questions_2.append(new_question)\n",
    "        \n",
    "        df_2['vocab_index'] = [[vocabulary[word] for word in question] for question in padded_questions_2]\n",
    "        return vocabulary_inv\n",
    "    \n",
    "    # Postag Index\n",
    "    if feature_type == 'pos_index':\n",
    "        MAX_WORDS = 20\n",
    "        if type(df.pos[0]) == str:\n",
    "            new_pos = []\n",
    "            for pos_vec in df.pos:\n",
    "                new_pos.append(eval(pos_vec))\n",
    "            df.pos = new_pos\n",
    "        if type(df_2.pos[0]) == str:\n",
    "            new_pos = []\n",
    "            for pos_vec in df_2.pos:\n",
    "                new_pos.append(eval(pos_vec))\n",
    "            df_2.pos = new_pos\n",
    "        \n",
    "        pos2idx = {'X':0}\n",
    "        for row in df.pos.values:\n",
    "            for pos in row:\n",
    "                if pos not in pos2idx:\n",
    "                    pos2idx[pos] = len(pos2idx)\n",
    "\n",
    "        ret = []\n",
    "        for pos_vec in df_2.pos.values:\n",
    "            r = []\n",
    "            for i in range(MAX_WORDS):\n",
    "                if i < len(pos_vec):\n",
    "                    if pos_vec[i] in pos2idx:\n",
    "                        r.append(pos2idx[pos_vec[i]])\n",
    "                    else:\n",
    "                        r.append(0)\n",
    "                else:\n",
    "                    # PAD\n",
    "                    r.append(len(pos2idx))\n",
    "            ret.append(r)\n",
    "        df_2['pos_index'] = ret\n",
    "    \n",
    "    # Postag hot-encode\n",
    "    if feature_type == 'pos_hotencode':\n",
    "        MAX_WORDS = 20\n",
    "        if type(df.pos.values[0]) == str:\n",
    "            new_pos = []\n",
    "            for pos_vec in df.pos:\n",
    "                new_pos.append(eval(pos_vec))\n",
    "            df.pos = new_pos\n",
    "        if type(df_2.pos.values[0]) == str:\n",
    "            new_pos = []\n",
    "            for pos_vec in df_2.pos:\n",
    "                new_pos.append(eval(pos_vec))\n",
    "            df_2.pos = new_pos\n",
    "        \n",
    "        pos2idx = {'X':0}\n",
    "        for row in df.pos.values:\n",
    "            for pos in row:\n",
    "                if pos not in pos2idx:\n",
    "                    pos2idx[pos] = len(pos2idx)\n",
    "        identityPos = np.identity(len(pos2idx))\n",
    "        \n",
    "        ret = []\n",
    "        for pos_vec in df_2.pos.values:\n",
    "            r = []\n",
    "            for i in range(MAX_WORDS):\n",
    "                if i < len(pos_vec):\n",
    "                    if pos_vec[i] in pos2idx:\n",
    "                        r += list(identityPos[pos2idx[pos_vec[i]]])\n",
    "                    else:\n",
    "                        r += list(identityPos[0])\n",
    "                else:\n",
    "                    # PAD\n",
    "                    r += list(np.zeros(len(pos2idx)))\n",
    "            ret.append(r)\n",
    "        df_2['pos_hotencode'] = ret\n",
    "    \n",
    "    # Named entity recognition Index\n",
    "    if feature_type == 'ner_index':\n",
    "        MAX_WORDS = 20\n",
    "        if type(df.ner.values[0]) == str:\n",
    "            new_ner = []\n",
    "            for ner_vec in df.ner:\n",
    "                new_ner.append(eval(ner_vec))\n",
    "            df.ner = new_ner\n",
    "        if type(df_2.ner.values[0]) == str:\n",
    "            new_ner = []\n",
    "            for ner_vec in df_2.ner:\n",
    "                new_ner.append(eval(ner_vec))\n",
    "            df_2.ner = new_ner\n",
    "        \n",
    "        ner2idx = {'':0}\n",
    "        for row in df.ner.values:\n",
    "            for ner in row:\n",
    "                if ner not in ner2idx:\n",
    "                    ner2idx[ner] = len(ner2idx)\n",
    "\n",
    "        ret = []\n",
    "        for ner_vec in df_2.ner.values:\n",
    "            r = []\n",
    "            for i in range(MAX_WORDS):\n",
    "                if i < len(ner_vec):\n",
    "                    if ner_vec[i] in ner2idx:\n",
    "                        r.append(ner2idx[ner_vec[i]])\n",
    "                    else:\n",
    "                        r.append(0)\n",
    "                else:\n",
    "                    # PAD\n",
    "                    r.append(len(ner2idx))\n",
    "            ret.append(r)\n",
    "        df_2['ner_index'] = ret\n",
    "    \n",
    "    # Named entity recognition Hot-encode\n",
    "    if feature_type == 'ner_hotencode':\n",
    "        MAX_WORDS = 20\n",
    "        if type(df.ner.values[0]) == str:\n",
    "            new_ner = []\n",
    "            for ner_vec in df.ner:\n",
    "                new_ner.append(eval(ner_vec))\n",
    "            df.ner = new_ner\n",
    "        if type(df_2.ner.values[0]) == str:\n",
    "            new_ner = []\n",
    "            for ner_vec in df_2.ner:\n",
    "                new_ner.append(eval(ner_vec))\n",
    "            df_2.ner = new_ner\n",
    "        \n",
    "        ner2idx = {'X':0}\n",
    "        for row in df.ner.values:\n",
    "            for ner in row:\n",
    "                if ner not in ner2idx:\n",
    "                    ner2idx[ner] = len(ner2idx)\n",
    "        identityNer = np.identity(len(ner2idx))\n",
    "        \n",
    "        ret = []\n",
    "        for ner_vec in df_2.ner.values:\n",
    "            r = []\n",
    "            for i in range(MAX_WORDS):\n",
    "                if i < len(ner_vec):\n",
    "                    if ner_vec[i] in ner2idx:\n",
    "                        r += list(identityNer[ner2idx[ner_vec[i]]])\n",
    "                    else:\n",
    "                        r += list(identityNer[0])\n",
    "                else:\n",
    "                    # PAD\n",
    "                    r += list(np.zeros(len(ner2idx)))\n",
    "            ret.append(r)\n",
    "        df_2['ner_hotencode'] = ret\n",
    "\n",
    "    if feature_type == 'bert':\n",
    "        # requires: pip install pytorch-transformers\n",
    "        import torch\n",
    "        \n",
    "        bert_model_name = 'bert-base-multilingual-cased'\n",
    "        # Load pretrained model/tokenizer\n",
    "        if 'bert_tokenizer' not in cache:\n",
    "            print('load bert_tokenizer...')\n",
    "            from pytorch_transformers import BertTokenizer\n",
    "            cache['bert_tokenizer'] = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        if 'bert_model' not in cache: \n",
    "            print('load bert_model...')\n",
    "            from pytorch_transformers import BertModel\n",
    "            cache['bert_model'] = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        # Encode text\n",
    "        print('tokenize...')\n",
    "        input_ids = [cache['bert_tokenizer'].encode(s) for s in df_2['question']]\n",
    "        input_ids_padded = pad_sequences(input_ids, maxlen=12, dtype='int64', padding='post', truncating='post', value=0)\n",
    "        input_ids_tensor = torch.tensor(input_ids_padded)\n",
    "        print('embed with BERT...')\n",
    "        with torch.no_grad():\n",
    "            ret = []\n",
    "            batch_size = 32\n",
    "            for ind in tqdm(range(0, len(input_ids), batch_size)):\n",
    "                batch_input = input_ids_tensor[ind:ind+batch_size]\n",
    "                last_hidden_states = cache['bert_model'](batch_input)[0]  # Models outputs are now tuples\n",
    "                ret.append(last_hidden_states.numpy())\n",
    "            ret = np.concatenate(ret, axis=0)\n",
    "            print(f'shape of encoded input: {ret.shape}')\n",
    "            df_2['bert'] = [enc for enc in ret]\n",
    "            \n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word_embedding):\n",
    "        self.word_embedding = word_embedding\n",
    "        self.dim = len(list(embedding.values())[0])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        ret = np.array([\n",
    "            np.sum([self.word_embedding[w] for w in words if w in self.word_embedding]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create classifier models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models are created through functions that return them. These functions will be used to create a new model in each experiment. Therefore, an instance of a model is created by the benchmark function and not explicitly in a code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear():\n",
    "    svc = LinearSVC(C=1.0)\n",
    "    return svc\n",
    "\n",
    "def lstm_default(in_dim=300, out_dim=7, drop=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_dim=in_dim, name='0_LSTM'))\n",
    "    model.add(Dropout(drop, name='1_Droupout'))\n",
    "    model.add(Dense(128, activation='relu', name='2_Dense'))\n",
    "    model.add(Dropout(drop, name='3_Droupout'))\n",
    "    model.add(Dense(out_dim, activation='softmax', name='4_Dense'))\n",
    "    #otimizer = keras.optimizers.Adam(lr=0.01) #decay = 0.0001\n",
    "    #model.compile(optimizer=otimizer, loss='categorical_crossentropy')\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "def random_forest():\n",
    "    return RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "def mlp(in_dim=5000, out_dim=7, drop=0.65):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=in_dim, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(out_dim, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def cnn(sequence_length, vocabulary_size, embedding_dim=300, filter_sizes=[3,4,5], num_filters=512, drop=0.5, out_dim=7):\n",
    "    inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "    embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "    reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "    \n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "    maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "    maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "    maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "    \n",
    "    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "    flatten = Flatten()(concatenated_tensor)\n",
    "    dropout = Dropout(drop)(flatten)\n",
    "    output = Dense(units=out_dim, activation='softmax')(dropout)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy')\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(emb_path, nmax=50000):\n",
    "    embedding = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in embedding, 'word found twice'\n",
    "            embedding[word] = vect\n",
    "            if len(embedding) == nmax:\n",
    "                break\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load UIUC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uiuc(language):\n",
    "    # language: 'en', 'pt' or 'es'\n",
    "    return pd.read_csv('datasets/UIUC_' + language + '/train_features.csv'), pd.read_csv('datasets/UIUC_' + language + '/test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load DISEQuA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_disequa(language):\n",
    "    df = pd.read_csv('datasets/DISEQuA/disequa_features.csv')\n",
    "    return df[df['language'] == language]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark UIUC - Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal:** it uses the default fixed split of UIUC between train dataset (at last 5500 instances) and test dataset (500 instances). Therefore, it does not use cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the *run_benchmark* function is executed, it will save each result in the *save* path.\n",
    "\n",
    "**model:** a dictionary with the classifier name and the function to create and return the model (not an instance of the model). <br> Example: *model = {'name': 'SVM', 'model': svm_linear}*<br>\n",
    "**X:** all the training set.<br>\n",
    "**y:** all the labels of the training set.<br>\n",
    "**x_test:** test set.<br>\n",
    "**y_test:** labels of the test set.<br>\n",
    "**sizes_train:** sizes of training set. For each size, an experiment is executed.<br>\n",
    "**runs:** number of time that each experiment is executed (used in models which has parameters with random values, like weights in an ANN).<br>\n",
    "**save:** csv path where the results will be saved.<br>\n",
    "**metric_average:** used in f1, recall and precision metrics<br>\n",
    "**onehot:** one-hot model to transform labels.<br>\n",
    "**out_dim:** the total of classes for ANN models.<br>\n",
    "**epochs:** epochs for ANN models.<br>\n",
    "**batch_size:** batch_size for ANN models.<br>\n",
    "**vocabulary_size:** vocabulary size (used in CNN model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model, X, y, x_test, y_test, sizes_train, runs=30, save='default.csv', \n",
    "                  metric_average=\"macro\", onehot=None, out_dim=6, epochs=10, batch_size=30, \n",
    "                  vocabulary_size=5000, in_dim=300):\n",
    "    start_benchmark = time.time()\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for size_train in sizes_train:\n",
    "\n",
    "        print('\\n'+str(size_train), end='|')\n",
    "\n",
    "        for run in range(runs):\n",
    "            print('.', end='')\n",
    "            x_train = X[:size_train]\n",
    "            y_train = y[:size_train]\n",
    "\n",
    "            if 'lstm' in model['name'] or 'mlp' in model['name']:\n",
    "                m = model['model'](in_dim=in_dim, out_dim=len(onehot.categories_[0]))\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=epochs, batch_size=batch_size)\n",
    "                train_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                result = np.nan_to_num(result)\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test_ = onehot.inverse_transform(y_test)\n",
    "            elif 'cnn' in model['name']:\n",
    "                sequence_length = x_train.shape[1]\n",
    "                out_dim = len(onehot.categories_[0])\n",
    "                m = model['model'](sequence_length, vocabulary_size, out_dim=out_dim)\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=epochs, batch_size=batch_size)\n",
    "                train_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                result = np.nan_to_num(result)\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test_ = onehot.inverse_transform(y_test)\n",
    "            else:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                y_test_ = y_test\n",
    "\n",
    "            data = {'datetime': datetime.datetime.now(),\n",
    "                    'model': model['name'],\n",
    "                    'accuracy': accuracy_score(result, y_test_),\n",
    "                    'precision': precision_score(result, y_test_, average=metric_average),\n",
    "                    'recall': recall_score(result, y_test_, average=metric_average),\n",
    "                    'f1': f1_score(result, y_test_, average=metric_average),\n",
    "                    'mcc': matthews_corrcoef(result, y_test_),\n",
    "                    'confusion': confusion_matrix(result, y_test_),\n",
    "                    'run': run + 1,\n",
    "                    'train_size': size_train,\n",
    "                    'execution_time': train_time,\n",
    "                    'test_time': test_time}\n",
    "            results = results.append([data])\n",
    "            results.to_csv(save)\n",
    "    print('')\n",
    "    aux = time.time() - start_benchmark\n",
    "    print('Run time benchmark:', aux)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark UIUC and DISEQuA - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-validation:** instead of uses default fixed splits, it uses the all the dataset with cross-validation with 10 folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the *run_benchmark* function is executed, it will save each result in the *save* path.\n",
    "\n",
    "**model:** a dictionary with the classifier name and the function to create and return the model (not an instance of the model). <br> Example: *model = {'name': 'SVM', 'model': svm_linear}*<br>\n",
    "**X:** Input features.<br>\n",
    "**y:** Input labels.<br>\n",
    "**sizes_train:** sizes of training set. For each size, an experiment is executed.<br>\n",
    "**folds:** Amount of folds for cross-validations.<br>\n",
    "**save:** csv path where the results will be saved.<br>\n",
    "**metric_average:** used in f1, recall and precision metrics<br>\n",
    "**onehot:** one-hot model to transform labels.<br>\n",
    "**epochs:** epochs for ANN models.<br>\n",
    "**batch_size:** batch_size for ANN models.<br>\n",
    "**vocabulary_size:** vocabulary size (used in CNN model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_cv(model, X, y, sizes_train, folds=10, save='default.csv', metric_average=\"macro\",\n",
    "                  onehot=None, epochs=10, batch_size=30, vocabulary_size=5000):\n",
    "    start_benchmark = time.time()\n",
    "    results = pd.DataFrame()\n",
    "    for size_train in sizes_train:\n",
    "        print('\\n'+str(size_train)+'|', end='')\n",
    "        size_test = len(X) - size_train\n",
    "        # StratifiedShuffleSplit maybe use it insted\n",
    "        rs = StratifiedShuffleSplit(n_splits=folds, train_size=size_train, \n",
    "                                    test_size=size_test, random_state=1)\n",
    "        fold = 0\n",
    "        for train_indexs, test_indexs in rs.split(X, y):\n",
    "            print('.', end='')\n",
    "            x_train = X[train_indexs]\n",
    "            y_train = y[train_indexs]\n",
    "            x_test = X[test_indexs]\n",
    "            y_test = y[test_indexs]\n",
    "\n",
    "            if 'lstm' in model['name']:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=epochs)\n",
    "                train_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test = onehot.inverse_transform(y_test)\n",
    "            elif 'cnn' in model['name']:\n",
    "                sequence_length = x_train.shape[1]\n",
    "                out_dim = len(onehot.categories_[0])\n",
    "                m = model['model'](sequence_length, vocabulary_size, out_dim=out_dim)\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train, verbose=0, epochs=epochs, batch_size=batch_size)\n",
    "                train_time = time.time() - start_time\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "                result = np.nan_to_num(result)\n",
    "                result = onehot.inverse_transform(result)\n",
    "                y_test = onehot.inverse_transform(y_test)\n",
    "            else:\n",
    "                m = model['model']()\n",
    "                start_time = time.time()\n",
    "                m.fit(x_train, y_train)\n",
    "                train_time = time.time() - start_time\n",
    "\n",
    "                start_time = time.time()\n",
    "                result = m.predict(x_test)\n",
    "                test_time = time.time() - start_time\n",
    "            \n",
    "                \n",
    "            data = {'datetime': datetime.datetime.now(),\n",
    "                    'accuracy': accuracy_score(result, y_test),\n",
    "                    'precision': precision_score(result, y_test, average=metric_average),\n",
    "                    'recall': recall_score(result, y_test, average=metric_average),\n",
    "                    'f1': f1_score(result, y_test, average=metric_average),\n",
    "                    'mcc': matthews_corrcoef(result, y_test),\n",
    "                    'confusion': confusion_matrix(result, y_test),\n",
    "                    'train_size': size_train,\n",
    "                    'fold': fold,\n",
    "                    'execution_time': train_time,\n",
    "                    'test_time': test_time}\n",
    "            results = results.append([data])\n",
    "            results.to_csv(save)\n",
    "            fold += 1\n",
    "    print('')\n",
    "    aux = time.time() - start_benchmark\n",
    "    print('Run time benchmark:', aux)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UIUC Benchmark - Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different classifier models are tested with different dependency levels of external linguistic resources (Low, Medium and High)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 0.41161417961120605\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 0.5091326236724854\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 0.4507761001586914\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, max_features=2000)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, max_features=2000)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf_train = np.array([list(r) for r in dataset_train['tfidf'].values])\n",
    "    tfidf_test = np.array([list(r) for r in dataset_test['tfidf'].values])\n",
    "    tfidf_train = normalize(tfidf_train, norm='max')\n",
    "    tfidf_test = normalize(tfidf_test, norm='max')\n",
    "    \n",
    "    X_train = np.array([list(x) for x in dataset_train['tfidf'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['tfidf'].values])\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  save='results/UIUC_svm_tfidf_' + language + '.csv', runs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 12.742478370666504\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 14.3670494556427\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 13.426743984222412\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, max_features=2000)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, max_features=2000)\n",
    "    create_feature('embedding_sum', None, dataset_train, embedding)\n",
    "    create_feature('embedding_sum', None, dataset_test, embedding)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf_train = np.array([list(r) for r in dataset_train['tfidf'].values])\n",
    "    tfidf_test = np.array([list(r) for r in dataset_test['tfidf'].values])\n",
    "    tfidf_train = normalize(tfidf_train, norm='max')\n",
    "    tfidf_test = normalize(tfidf_test, norm='max')\n",
    "    \n",
    "    embedding_train = np.array([list(r) for r in dataset_train['embedding_sum'].values])\n",
    "    embedding_test = np.array([list(r) for r in dataset_test['embedding_sum'].values])\n",
    "    embedding_train = normalize(embedding_train, norm='max')\n",
    "    embedding_test = normalize(embedding_test, norm='max')\n",
    "    \n",
    "    X_train = np.array([list(x) + list(xx) for x, xx in zip(tfidf_train, embedding_train)])\n",
    "    X_test = np.array([list(x) + list(xx) for x, xx in zip(tfidf_test, embedding_test)])\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500], \n",
    "                  runs=1, save='results/UIUC_svm_cortes_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB + POS + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 12.932790994644165\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 15.48978304862976\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "1000|.\n",
      "2000|.\n",
      "3000|.\n",
      "4000|.\n",
      "5500|.\n",
      "Run time benchmark: 14.322027683258057\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    create_feature('tfidf', dataset_train, dataset_train, max_features=2000)\n",
    "    create_feature('tfidf', dataset_train, dataset_test, max_features=2000)\n",
    "    create_feature('embedding_sum', dataset_train, dataset_train, embedding)\n",
    "    create_feature('embedding_sum', dataset_train, dataset_test, embedding)\n",
    "    create_feature('pos_hotencode', dataset_train, dataset_train)\n",
    "    create_feature('pos_hotencode', dataset_train, dataset_test)\n",
    "    create_feature('ner_hotencode', dataset_train, dataset_train)\n",
    "    create_feature('ner_hotencode', dataset_train, dataset_test)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf_train = np.array([list(r) for r in dataset_train['tfidf'].values])\n",
    "    tfidf_test = np.array([list(r) for r in dataset_test['tfidf'].values])\n",
    "    tfidf_train = normalize(tfidf_train, norm='max')\n",
    "    tfidf_test = normalize(tfidf_test, norm='max')\n",
    "    \n",
    "    embedding_train = np.array([list(r) for r in dataset_train['embedding_sum'].values])\n",
    "    embedding_test = np.array([list(r) for r in dataset_test['embedding_sum'].values])\n",
    "    embedding_train = normalize(embedding_train, norm='max')\n",
    "    embedding_test = normalize(embedding_test, norm='max')\n",
    "    \n",
    "    pos_train = np.array([list(r) for r in dataset_train['pos_hotencode'].values])\n",
    "    pos_test = np.array([list(r) for r in dataset_test['pos_hotencode'].values])\n",
    "    \n",
    "    ner_train = np.array([list(r) for r in dataset_train['ner_hotencode'].values])\n",
    "    ner_test = np.array([list(r) for r in dataset_test['ner_hotencode'].values])\n",
    "    \n",
    "    X_train = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf_train, embedding_train, pos_train, ner_train)])\n",
    "    X_test = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf_test, embedding_test, pos_test, ner_test)])\n",
    "    \n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    \n",
    "    classes = list(dataset_train['class'].unique())\n",
    "    y_train_ = [classes.index(c) for c in y_train]\n",
    "    \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=1, save='results/UIUC_svm_high_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run UIUC Benchmark - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different classifier models are tested with different dependency levels of external linguistic resources (Low, Medium and High)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 22.216983318328857\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 24.743942499160767\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 22.218426942825317\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset = pd.concat([dataset_train, dataset_test])\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, [50, 100] + list(range(500, 5501, 500)),\n",
    "                     save='results/UIUC_cv_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 270.81999158859253\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 329.85390615463257\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 327.71513843536377\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset = pd.concat([dataset_train, dataset_test])\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', None, dataset, embedding)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) for x, xx in zip(tfidf, embedding)])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, [50, 100] + list(range(500, 5501, 500)),\n",
    "                     save='results/UIUC_cv_svm_cortes_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + TF-IDF + WB + POS + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 300.1998710632324\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 361.68490052223206\n",
      "\n",
      "\n",
      "Language:  pt\n",
      "\n",
      "50|..........\n",
      "100|..........\n",
      "500|..........\n",
      "1000|..........\n",
      "1500|..........\n",
      "2000|..........\n",
      "2500|..........\n",
      "3000|..........\n",
      "3500|..........\n",
      "4000|..........\n",
      "4500|..........\n",
      "5000|..........\n",
      "5500|..........\n",
      "Run time benchmark: 308.6705446243286\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'pt']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset = pd.concat([dataset_train, dataset_test])\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', dataset, dataset, embedding)\n",
    "    create_feature('pos_hotencode', dataset, dataset)\n",
    "    create_feature('ner_hotencode', dataset, dataset)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    pos = np.array([list(r) for r in dataset['pos_hotencode'].values])\n",
    "    \n",
    "    ner = np.array([list(r) for r in dataset['ner_hotencode'].values])\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf, embedding, pos, ner)])\n",
    "    \n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, [50, 100] + list(range(500, 5501, 500)),\n",
    "                     save='results/UIUC_cv_svm_high_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DISEQuA Benchmark - Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different classifier models are tested with different dependency levels of external linguistic resources (Low, Medium and High)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + <font color=#007700>TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.027012586593628\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.0114972591400146\n",
      "\n",
      "\n",
      "Language:  it\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.1434721946716309\n",
      "\n",
      "\n",
      "Language:  nl\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 1.1250619888305664\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'it', 'nl']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, sizes_train=[100,200,300,400],\n",
    "                     save='results/DISEQuA_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + <font color=#007700>TF-IDF</font> + <font color=#0055CC>WB</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.358882427215576\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 7.197380065917969\n",
      "\n",
      "\n",
      "Language:  it\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 5.5334153175354\n",
      "\n",
      "\n",
      "Language:  nl\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.624628782272339\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'it', 'nl']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', None, dataset, embedding)\n",
    "    \n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) for x, xx in zip(tfidf, embedding)])\n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, sizes_train=[100,200,300,400],\n",
    "                     save='results/DISEQuA_svm_cortes_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + <font color=#007700>TF-IDF</font> + <font color=#0055CC>WB</font> + <font color=#CC6600>POS</font> + <font color=#CC6600>NER</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.811999559402466\n",
      "\n",
      "\n",
      "Language:  es\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 8.384974479675293\n",
      "\n",
      "\n",
      "Language:  it\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.426969528198242\n",
      "\n",
      "\n",
      "Language:  nl\n",
      "\n",
      "100|..........\n",
      "200|..........\n",
      "300|..........\n",
      "400|..........\n",
      "Run time benchmark: 6.852076053619385\n"
     ]
    }
   ],
   "source": [
    "for language in ['en', 'es', 'it', 'nl']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, max_features=2000)\n",
    "    create_feature('embedding_sum', dataset, dataset, embedding)\n",
    "    create_feature('pos_hotencode', dataset, dataset)\n",
    "    create_feature('ner_hotencode', dataset, dataset)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    \n",
    "    tfidf = np.array([list(r) for r in dataset['tfidf'].values])\n",
    "    tfidf = normalize(tfidf, norm='max')\n",
    "    \n",
    "    embedding = np.array([list(r) for r in dataset['embedding_sum'].values])\n",
    "    embedding = normalize(embedding, norm='max')\n",
    "    \n",
    "    pos = np.array([list(r) for r in dataset['pos_hotencode'].values])\n",
    "    \n",
    "    ner = np.array([list(r) for r in dataset['ner_hotencode'].values])\n",
    "    \n",
    "    X = np.array([list(x) + list(xx) + list(xxx) + list(xxxx) for x, xx, xxx, xxxx in zip(tfidf, embedding, pos, ner)])\n",
    "    \n",
    "    y = dataset['class'].values\n",
    "    \n",
    "    run_benchmark_cv(model, X, y, sizes_train=[100,200,300,400],\n",
    "                     save='results/DISEQuA_svm_high_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old stuffs bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'en', 'es'\n",
    "for language in ['es']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    #embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    text_representation = 'vocab_index'\n",
    "    vocabulary_inv = create_feature(text_representation, dataset_train, dataset_train)\n",
    "    create_feature(text_representation, dataset_train, dataset_test)\n",
    "    model = {'name': 'cnn', 'model': cnn}\n",
    "    X_train = np.array([list(x) for x in dataset_train[text_representation].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test[text_representation].values])\n",
    "    #X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    #X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray()\n",
    "    # , \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=30, save='results/UIUC_cnn_' + language + '.csv', epochs=100, onehot=ohe,\n",
    "                  vocabulary_size=len(vocabulary_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  es\n",
      "(100,)\n",
      "object\n",
      "(1349,)\n",
      "\n",
      "1000|...\n",
      "2000|...\n",
      "3000|...\n",
      "4000|...\n",
      "5500|...\n",
      "Run time benchmark: 228.79835891723633\n"
     ]
    }
   ],
   "source": [
    "for language in ['es']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    dataset_train = dataset_train[:100]\n",
    "    #dataset_test = dataset_test[:10]\n",
    "    create_feature('embedding', dataset_train, dataset_train, embedding)\n",
    "    create_feature('embedding', dataset_train, dataset_test, embedding)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    #print(dataset_train['embedding'].values.shape)\n",
    "    #print(dataset_train['embedding'].values.dtype)\n",
    "    #print(dataset_test['embedding'].values.shape)\n",
    "    X_train = np.array([list(x) for x in dataset_train['embedding'].values])\n",
    "    X_test = np.array([list(x) for x in dataset_test['embedding'].values])\n",
    "    X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "#     y_train_sub = dataset_train['sub_class'].values\n",
    "#     sub_classes = set()\n",
    "#     for sc in y_train_sub:\n",
    "#         sub_classes.add(sc)\n",
    "#     y_test_sub = dataset_test['sub_class'].values\n",
    "#     X_test_sub_ = []\n",
    "#     y_test_sub_ = []\n",
    "#     for i in range(len(X_test)):\n",
    "#         if y_train_sub[i] in sub_classes:\n",
    "#             X_test_sub_.append(X_test[i])\n",
    "#             y_test_sub_.append(y_train_sub[i])\n",
    "#     X_test_sub_ = np.array(X_test_sub_)\n",
    "#     y_test_sub_ = np.array(y_test_sub_)\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray() \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "                  runs=30, save='results/UIUC_lstm_embedding_' + language + '_2.csv', epochs=100, onehot=ohe)\n",
    "    #run_benchmark(model, X_train, y_train_sub, X_test_sub_, y_test_sub_, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "    #              save='results/UIUCsub_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Language:  en\n",
      "WARNING: use subset (first 1000 entries) of training data\n",
      "tokenize...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed with BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 171/171 [00:50<00:00,  4.03it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded input: (5452, 12, 768)\n",
      "tokenize...\n",
      "embed with BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 16/16 [00:04<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoded input: (500, 12, 768)\n",
      "\n",
      "5500|."
     ]
    }
   ],
   "source": [
    "for language in ['en']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    #embedding = load_embedding(path_wordembedding + 'wiki.multi.' + language + '.vec')\n",
    "    dataset_train, dataset_test = load_uiuc(language)\n",
    "    # debug\n",
    "    print('WARNING: use subset (first 1000 entries) of training data')\n",
    "    #dataset_train = dataset_train[:5500].copy()\n",
    "    \n",
    "    create_feature('bert', dataset_train, dataset_train)\n",
    "    create_feature('bert', dataset_train, dataset_test)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    X_train = dataset_train['bert'].values\n",
    "    X_test = dataset_test['bert'].values\n",
    "    \n",
    "    X_train = np.array([x for x in X_train])\n",
    "    X_test = np.array([x for x in X_test])\n",
    "    \n",
    "    #X_train = pad_sequences(X_train, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    #X_test = pad_sequences(X_test, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    y_train = dataset_train['class'].values\n",
    "    y_test = dataset_test['class'].values\n",
    "    ohe = OneHotEncoder()\n",
    "    y_train = ohe.fit_transform([[y_] for y_ in y_train]).toarray()\n",
    "    y_test = ohe.transform([[y_] for y_ in y_test]).toarray() \n",
    "    run_benchmark(model, X_train, y_train, X_test, y_test, sizes_train=[5500], # 1000, 2000, 3000, 4000, 5500\n",
    "                  runs=1, save='results/UIUC_lstm_bert_' + language + '.csv', \n",
    "                  epochs=100, onehot=ohe, in_dim=768)\n",
    "    #run_benchmark(model, X_train, y_train_sub, X_test_sub_, y_test_sub_, sizes_train=[1000, 2000, 3000, 4000, 5500],\n",
    "    #              save='results/UIUCsub_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN DISEQuA Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, embedding)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400,405],\n",
    "                  save='results/DISEQuA_svm_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFC + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf', dataset, dataset, embedding)\n",
    "    model = {'name': 'rfc', 'model': random_forest}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_rfc_tfidf_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM + TFIDF_3gram + SKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in ['DUT', 'ENG', 'ITA', 'SPA']:\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('tfidf_3gram', dataset, dataset)\n",
    "    model = {'name': 'svm', 'model': svm_linear}\n",
    "    X = np.array([list(x) for x in dataset['tfidf'].values])\n",
    "    y = dataset['class'].values\n",
    "    skb = SelectKBest(chi2, k=2000).fit(X, y)\n",
    "    X = skb.transform(X)\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400],\n",
    "                  save='results/DISEQuA_svm_tfidf_3gram_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language, embd_l in zip(['SPA'], ['es']):\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    embedding = load_embedding(path_wordembedding + 'wiki.multi.' + embd_l + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    create_feature('embedding', dataset, dataset, embedding)\n",
    "    model = {'name': 'lstm', 'model': lstm_default}\n",
    "    X = np.array([list(x) for x in dataset['embedding'].values])\n",
    "    y = dataset['class'].values\n",
    "    X = pad_sequences(X, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    ohe = OneHotEncoder()\n",
    "    y = ohe.fit_transform([[y_] for y_ in y]).toarray()\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400,405], onehot=ohe,\n",
    "                  save='results/DISEQuA_lstm_embedding_' + language + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language, embd_l in zip(['DUT', 'ENG', 'ITA', 'SPA'], ['nl', 'eng', 'it', 'es']):\n",
    "    print('\\n\\nLanguage: ', language)\n",
    "    #embedding = load_embedding(path_wordembedding + 'wiki.multi.' + embd_l + '.vec')\n",
    "    dataset = load_disequa(language)\n",
    "    text_representation = 'vocab_index'\n",
    "    vocabulary_inv = create_feature(text_representation, dataset, dataset)\n",
    "    model = {'name': 'cnn', 'model': cnn}\n",
    "    X = np.array([list(x) for x in dataset[text_representation].values])\n",
    "    y = dataset['class'].values\n",
    "    #X = pad_sequences(X, maxlen=12, dtype='float', padding='post', truncating='post', value=0.0)\n",
    "    ohe = OneHotEncoder()\n",
    "    y = ohe.fit_transform([[y_] for y_ in y]).toarray()\n",
    "    run_benchmark(model, X, y, sizes_train=[100,200,300,400], onehot=ohe, vocabulary_size=len(vocabulary_inv),\n",
    "                  save='results/DISEQuA_cnn_' + language + '.csv', epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "qa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
